# Deep Learning-Based Framework for Efficient Design of Multicomponent High Hardness High Entropy Alloys

## 1. 项目概述

本代码是一个用于材料信息学研究，主要包含两大核心功能：

1. **命名实体识别 (NER)**：利用深度学习模型从材料科学的科学文献中自动抽取出关键信息，例如材料名称、制备方法、性能参数、元素组成等。
2. **高熵合金 (HEA) 设计**：通过遗传算法 (GA) 结合机器学习预测模型，自动化地探索和设计具有特定目标性能（如高硬度）的新型高熵合金成分。

这两个模块协同工作，形成了一个从非结构化文本中提取知识，并利用这些知识指导新材料设计的完整流程。

---

## 2. 系统架构

本项目的系统由两个相对独立但功能上互补的模块组成：

- **命名实体识别 (NER) 模块**

  - **输入**: 包含科学文献文本的 `.jsonl` 文件。
  - **处理**: 使用一个集成了 **SciBERT**、**FastText**、**BiLSTM** 和 **CRF** 的复合模型 (`SciBert_FastText_BiLSTM_CRF`) 对文本进行序列标注。
  - **输出**: 从文本中识别并提取出的实体，最终保存为 `.csv` 格式的文件。

- **高熵合金 (HEA) 设计模块**
  - **输入**: 包含已知高熵合金元素配比及其对应硬度值的 `.xlsx` 文件。
  - **处理**: 首先，基于输入数据训练一个硬度预测模型 (如 `XGBoost`)。 然后，一个遗传算法将此预测模型作为其适应度函数，通过模拟“进化”过程（选择、交叉、变异）来寻找能够使预测硬度最大化的合金配比方案。
  - **输出**: 一系列具有高预测硬度的、有潜力的新型高熵合金配方，保存在 `.json` 文件中。

---

## 3. 核心模块详解

### 3.1. 命名实体识别 (NER) 模块

该模块旨在从非结构化的文本中精准地抽取出结构化的实体信息。

#### **模型架构 (`model.py`)**

模型采用 `SciBert_FastText_BiLSTM_CRF` 架构，这是一种在序列标注任务中表现优异的经典复合模型。

- **SciBERT + FastText**:
  - **SciBERT**: 使用在大量科学文献上预训练的 BERT 模型 (`matscibert`)，以生成具有丰富上下文信息的词向量。
  - **FastText**: 引入预训练的 FastText 词向量作为补充，增强模型对词汇形态的理解。
- **BiLSTM (双向长短期记忆网络)**:
  - 在 BERT 和 FastText 生成的词向量之上，使用 BiLSTM 网络从正向和反向两个维度捕捉文本的序列依赖关系，进一步增强特征表示。
- **CRF (条件随机场)**:
  - 作为模型的最后一层，CRF 层可以学习到标签之间的转移约束（例如，`I-Name` 标签必须跟在 `B-Name` 之后），确保输出的标签序列在语法上是合规的，从而有效提升标注的准确率。

#### **训练流程 (`bert_w2v_main.py`)**

此脚本负责模型的训练和评估。

1. **数据加载**: 从 `.jsonl` 文件 (`Paper_HEA_train.jsonl`, `Paper_HEA_test.jsonl`) 中读取训练和测试数据。 `dataset.py` 中的 `SLSDataset` 类负责将文本和标签转换为模型所需的张量格式。
2. **模型初始化**: 加载 `SciBert_FastText_BiLSTM_CRF` 模型，并使用 `Adam` 优化器。
3. **迭代训练**:
    - 在设定的 `epoch` 数量内进行循环训练。
    - 在每个 `epoch` 中，模型在训练集上进行训练，并通过反向传播更新权重。
    - 训练完成后，在测试集上进行评估。
4. **性能评估**: 使用 `metrics.py` 中的 `Score` 类计算每个 `epoch` 的**精确率 (Precision)**、**召回率 (Recall)** 和 **F1 分数**。
5. **结果保存**:
    - 每个 `epoch` 的详细评估结果会被实时记录到 `.json` 日志文件中。
    - 所有 `epoch` 的核心指标（F1, P, R）最终会汇总并保存到一个 `.csv` 文件中，便于分析模型的收敛过程。

#### **推理流程 (`inference.py`)**

此脚本使用训练好的模型对新的文本数据进行实体提取。

1. **加载模型**: 加载已经训练并保存好的模型权重。
2. **处理新数据**: 从 `ner_passage_HEA.json` 文件中加载待处理的文本。
3. **执行预测**: 模型对输入文本进行预测，输出一个标签 ID 序列。
4. **后处理**:
    - `process_sample`: 将 BERT 分词器产生的子词 (sub-word) 的预测结果重新对齐到原始的单词上。
    - `extract_entities`: 根据 `B-I-O` 标注规范，从单词和其对应的预测标签中抽取出完整的实体。
5. **保存结果**: 将所有抽取出的实体写入到 `output.csv` 文件中。

### 3.2. 高熵合金 (HEA) 设计模块

该模块利用遗传算法来智能地设计新型高熵合金。

#### **硬度预测模型 (`Pre_Hardness.py`, `HEA_Design.py`)**

遗传算法的“智能”来源于其评估个体优劣的能力，这种能力由一个机器学习模型提供。

- `Pre_Hardness.py`: 该脚本用于硬度预测模型的预研究。 它从 `material_compositions.xlsx` 读取数据，并使用交叉验证来评估不同回归模型（如 SVR）的性能（MAE, R²），为选择最终模型提供依据。
- `HEA_Design.py`: 在该脚本中，最终选用了一个 `XGBoost` 回归器作为硬度预测模型。 它在整个数据集上进行训练，并被封装为遗传算法的**适应度函数 (Fitness Function)**。

#### **遗传算法 (GA) 流程 (`HEA_Design.py`)**

此脚本是合金设计的核心，基于 `deap` 库实现。

1. **初始化**:
    - **个体 (Individual)**: 每个个体代表一种合金配方，是一个包含 13 个元素的原子百分比的列表，总和为 100。 初始个体会随机生成含有 5 种元素的组合。
    - **种群 (Population)**: 由多个（例如 50 个）随机生成的个体组成。
2. **进化循环**:
    - **评估 (Evaluation)**: 对种群中的每一个体，使用预训练好的 `XGBoost` 模型预测其硬度值。 这个硬度值就是该个体的**适应度**。
    - **选择 (Selection)**: 根据适应度值的高低，选择优良的个体进入下一代。 适应度越高的个体被选中的概率越大（采用锦标赛选择法 `selTournament`）。
    - **交叉 (Crossover)**: 模仿生物的基因重组，将两个父代个体的元素配比进行部分交换，生成新的子代。
    - **变异 (Mutation)**: 以一定的概率随机改变个体中的某个元素配比，为种群引入新的多样性，有助于跳出局部最优解。
3. **迭代与结果输出**:
    - 以上进化过程会重复执行指定的代数（例如 100 代）。
    - 整个流程会重复运行上千次，以充分探索解空间。
    - 在每次运行结束后，如果发现的最优个体其预测硬度超过一个阈值（例如 600 HV），则其配方和预测硬度值将被保存到 `result_genetic_algorithm_results_400.json` 文件中。

---

## 4. 文件结构说明

```text
.
├── HEA_Design.py             # 高熵合金遗传算法设计主脚本
├── Pre_Hardness.py           # 硬度预测模型预研与评估脚本
├── bert_w2v_main.py          # NER模型训练主脚本
├── inference.py              # NER模型推理主脚本
│
└── package/                  # 模型和工具包
    ├── model.py              # 定义SciBert_FastText_BiLSTM_CRF模型架构
    ├── dataset.py            # PyTorch数据集类，用于加载NER数据
    ├── dataset_word.py       # (功能与dataset.py类似，可能是备份或早期版本)
    ├── nn.py                 # 条件随机场(CRF)层的具体实现
    ├── metrics.py            # NER评估指标（F1, P, R）的计算类
    └── utils.py              # 工具函数，如读取json、解析BIO标签等
```

---

## 5. 环境设置与安装

在运行代码前，请确保安装了所有必要的 Python 库。

```bash
pip install torch transformers gensim pandas scikit-learn xgboost deap tqdm
```

此外，您需要下载预训练模型并放置在指定的路径下。代码中引用的路径为示例，请根据您的实际存储位置进行修改：

- **MatSciBERT 模型**: 下载并放置在如 `/root/autodl-tmp/NER-SciBERT/resource/matscibert` 的路径。
- **FastText 词向量**: 下载 `fasttext_embeddings-MINIFIED.model` 并放置在如 `/root/autodl-tmp/NER-SciBERT/fasttext/` 的路径。

---

## 6. 使用说明

### **数据准备**

- **NER 数据**: 准备 `.jsonl` 格式的数据文件。 每一行是一个 JSON 对象，包含 `"words"` (一个单词列表) 和 `"ner"` (一个对应的 BIO 标签列表)。
- **硬度数据**: 准备 `.xlsx` 格式的 Excel 文件。 其中列为元素名称（如`Al`, `Co`, `Cr`...），行为不同的合金样品，最后一列为对应的硬度值 `HV`。

### **训练 NER 模型**

1. 确认 `bert_w2v_main.py` 中的文件路径（数据集、预训练模型）已正确配置。
2. 运行以下命令开始训练：`python bert_w2v_main.py`
3. 训练过程将在控制台显示进度条。 训练完成后，将在同级目录下生成包含评估结果的 `.csv` 和 `.json` 文件。

### **执行 NER 推理**

1. 确保您已经有了一个训练好的模型权重文件（`.pth`）。
2. 修改 `inference.py`，将 `torch.load()` 中的路径指向您的模型权重文件，并配置好输入文件 (`ner_passage_HEA.json`) 的路径。
3. 运行以下命令：`python inference.py`
4. 脚本执行完毕后，会在同级目录下生成一个 `output.csv` 文件，其中包含从文本中提取出的所有实体。

### **运行 HEA 遗传算法**

1. 确认 `HEA_Design.py` 中的 Excel 数据文件路径正确无误。
2. 运行以下命令启动遗传算法：`python HEA_Design.py`
3. 算法会长时间运行，并在控制台打印每一代的进化信息。 最终，满足条件的高硬度合金配方将被追加到指定的 `.json` 结果文件中。
